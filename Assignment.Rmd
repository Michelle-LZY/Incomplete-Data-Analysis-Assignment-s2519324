---
editor_options:
  markdown:
    wrap: 72
title: "Incomplete Data Analysis Assingment"
author: Zheyue Lin(s2519324)
output:
  pdf_document:
    latex_engine: xelatex
date: "2024-03-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```
# Question 1
## (a) Density function of z and the frequency function of δ
Since $Z=min\{X,Y\}$, also $X$ and $Y$ are independent. So $Pr(Z>z)=Pr(min\{X,Y\}>z)=Pr(X>z,\ Y>z)=Pr(X>z)Pr(Y>z)=(1-F_X(z))(1-F_Y(z))=\frac{1}{z^{\mu+\lambda}}$. So we got the cumulative distribution function of $Z$ to be $$F_Z(z)=1-Pr(Z>z)=1-\frac{1}{z^{\mu+\lambda}}$$
where $\lambda>0, \mu>0, z\ge 1$. This cumulative distribution function also indicates $Z$ follows **Pareto distribution**.

Probability density function of $Z$ will be:
$$
f_Z(z)=\frac{dF_Z(z)}{dz}=\frac{\lambda+\mu}{z^{\mu+\lambda+1}}
$$
Let's look at the frequency function of $\delta$. Since $X$ and $Y$ are independent, $f_{XY}(x,y)=f_X(x)f_Y(y)$
\begin{align*}
Pr(\delta = 1) & = Pr(X<Y)\\
&=\int \limits_{1}^{\infty}\int \limits_{1}^{y}f_{XY}(x,y)dxdy\\
&=\int \limits_{1}^{\infty}\int \limits_{1}^{y}f_{X}(x)f_Y(y)dxdy\\
&= \int \limits_{1}^{\infty}F_x(y)f_Y(y)dy\\
&= \int \limits_{1}^{\infty}(1-\frac{1}{y^{\lambda}})\mu y^{-(\mu+1)}dy\\
&= 1+\frac{\mu}{\mu+\lambda}y^{-(\mu+\lambda)}|^{\infty}_{1}\\
&= \frac{\lambda }{\mu+\lambda }\\
\end{align*}
$Pr(\delta=0)=1-Pr(\delta=1)=\frac{\mu}{\mu+\lambda}$. So $\delta$ follows **Bernoulli distribution**, equally,  $\delta_i\stackrel{iid}{\sim}Bernoulli(\frac{\lambda }{\mu+\lambda})$, where $\lambda>0, \mu>0$.

## (b)Derive the maximum liklihood estimators of θ and p
Since $\theta=\mu+\lambda,\ p=\frac{\lambda}{\lambda+\mu}$, the probability density function of $z$ becomes
$$
f_Z(z)=(\lambda+\mu)\frac{1}{z^{\mu+\lambda+1}}=\frac{\theta}{z^{(\theta+1)}}
$$
where $z\ge1,\theta>0$. 

Also, the frequency functions of $\delta$ become $Pr(\delta=1)=p,\ Pr(\delta=0)=1-p$ where $0<p<1$.

Then, likelihood function and log-likelihood function for $\theta$ and $p$ will be:
$$
L(\theta;z)=\prod_{i}f_Z(z)=\theta^{n}\prod_{i}z_i^{-(\theta+1)}
$$
$$
\log L(\theta;z)=n\log\theta-(\theta+1)\sum_{i}\log z_i
$$
$$
L(p;\delta)=p^{\sum_i\delta_i}(1-p)^{\sum_i(1-\delta_i)}
$$
$$
\log L(p;\delta)=\sum_{i}\{\delta_i\log p+(1-\delta_i)\log (1-p)\}
$$
Taking first order derivative of log-likelihood functions, score functions of $\theta$ and $p$ will be
$$
U(\theta)=\frac{d\log L(\theta)}{d\theta}=\frac{n}{\theta}-\sum_i \log z_i,& U(p)=\frac{d\log L(\delta)}{d\delta}=\frac{\sum_i\delta_i}{p}-\frac{\sum_i(1-\delta_i)}{1-p}
$$
Let $U(\theta)=0, U(\delta)=0$, we have $\hat{\theta}=\frac{n}{\sum_i \log z_i}$, $\hat{p}=\frac{\sum_i\delta_i}{n}$.

For further verification, we also calculate the Fisher Information of $\theta$ and $p$:
$$
I(\theta)=E(-\frac{dU(\theta)}{d\theta})=E(\frac{n}{\theta^2})=\frac{n}{\theta^2}>0
$$
$$
I(p)=E(-\frac{dU(p)}{dp})=E(\frac{\sum_i\delta_i}{p^2}+\frac{n-\sum_i\delta_i}{(1-p)^2})=\frac{\sum_iE(\delta_i)}{p^2}+\frac{n-\sum_iE(\delta_i)}{(1-p)^2}=\frac{n}{p(1-p)}>0
$$
Both $\theta$ and $p$ have positive Fisher Information. Thus, $\hat{\theta}=\frac{n}{\sum_i \log z_i}$ is the maximum likelihood estimator of $\theta$ and $\hat{p}=\frac{\sum_i\delta_i}{n}$ is the maximum likelihood estimator of $p$.

## (c) 95% Confidence Interval
Combining Fisher Information and maximum likelihood estimates we have got from (2), we can calculate the standard error of the estimates
$$
SE(\hat{\theta})=\sqrt{I(\hat{\theta})^{-1}}=\frac{\sqrt{n}}{\sum_i\log z_i},&
SE(\hat{p})=\sqrt{I(\hat{p})^{-1}}=\frac{1}{n}\sqrt{(\sum_i\delta_i)(1-\frac{\sum_i\delta_i}{n})}
$$
Then we can construct 95% confidence interval for the MLEs.

$CI_{\hat{\theta}}$: $[\hat{\theta}\pm\alpha_{95\%}SE(\hat{\theta})]=[\frac{n}{\sum_i\log z_i}\pm\alpha_{95\%}\frac{\sqrt{n}}{\sum_i\log z_i}]$; 
$CI_{\hat{p}}$: $[\hat{p}\pm\alpha_{95\%}SE(\hat{p})]=[\frac{\sum_i\delta_i}{n}\pm\frac{\alpha_{95\%}}{n}\sqrt{(\sum_i\delta_i)(1-\frac{\sum_i\delta_i}{n})}]$
where $\alpha_{95\%}\approx 1.96$ is the quantile of the standard normal distribution for the 95% confidence interval.

# Question 2
In this question, we will use `mice` to perform multiple imputation and compute confidence intervals to check the difference on convergence of different imputation methods: `norm.nob` : (normal linear) stochastic regression imputation without uncertainty; `norm`; (normal linear) stochastic regression imputation considering uncertainty, `norm.boot`: bootstrap based (normal linear) stochastic regression imputation with uncertainty.
```{r}
require(mice)
load("dataex2.RData")

count_nob <- 0
count_norm <- 0
count_boot <- 0
for (i in 1:100){
  data <- data.frame("X" = dataex2[,"X",i], "Y" = dataex2[,"Y",i])
  
  # Stochastic Regression Imputation without considering uncertainty
  imp_nob <- mice(data, m = 20, "norm.nob", seed = 1, printFlag = FALSE)
  fit_nob <- with(imp_nob, lm(Y~X))
  pool_nob <- pool(fit_nob)
  conf_nob <- summary(pool_nob, conf.int = TRUE)[2, c(7,8)]
  if (conf_nob[1] <= 3 & 3 <= conf_nob[2]){
    count_nob = count_nob + 1
  }
  
  # Stochastic Regression Imputation considering uncertainty
  imp_norm <- mice(data, m = 20, "norm", seed = 1, printFlag = FALSE)
  fit_norm <- with(imp_norm, lm(Y~X))
  pool_norm <- pool(fit_norm)
  conf_norm <- summary(pool_norm, conf.int = TRUE)[2, c(7,8)]
  if (conf_norm[1] <= 3 & 3 <= conf_norm[2]){
    count_norm = count_norm + 1
  }
  
  # Regression Imputation using Bootstrap considering uncertainty
  imp_boot <- mice(data, m = 20, "norm.boot", seed = 1, printFlag = FALSE)
  fit_boot <- with(imp_boot, lm(Y~X))
  pool_boot <- pool(fit_boot)
  conf_boot <- summary(pool_boot, conf.int = TRUE)[2, c(7,8)]
  if (conf_boot[1] <= 3 & 3 <= conf_boot[2]){
    count_boot = count_boot + 1
  }
}
prob_nob <- count_nob/100
prob_norm <- count_norm/100
prob_boot <- count_boot/100

print(paste("prob_nob:",prob_nob))
print(paste("prob_norm:",prob_norm))
print(paste("prob_boot:",prob_boot))
```
According to the output, we have these findings:

1. Comparing the results of methods `norm.nob`(without uncertainty) and `norm`(considering uncertainty), stochastic regression imputation considering uncertainty gives higher empirical converge probabilities, showing that `norm.nob`(without uncertainty) may lead to narrower confidence intervals and further affecting corresponding converge of the intervals.

2.`norm` and `norm.boot` have the imputed values of the same forms but the estimates of regression parameters are calculated differently. Parameters in `norm` are drawn from the parameters' posterior distributions, while parameters in `norm.boot` are the least squares/maximum likelihood estimates from bootstrap sample of observed data.  These two methods give similar empirical converge probabilities, both are higher than the converge probability of the `norm.nob` method without uncertainty.

3. The difference between `norm` and `norm.boot` is smaller than the difference between `norm` and `norm.nob`, indicating that the parameter computation has less influence on the converge probability than whether considering uncertainty.

4.Bootstrap based method outperformed the other two methods on empirical converge probability.

# Question 3
## (a) Show the log likelihood of the observed data
Given $Y_i\stackrel{iid}{\sim}Normal(\mu,\sigma)$ for $i=1,2,...,n$, then the probability density function $f_Y(y_i)=\phi(y_i;\mu,\sigma^2)$, cumulative probability function $F_Y(y_i)=\Phi(y_i;\mu,\sigma^2)$.

Given $X_i$=\left\{\begin{matrix}
  Y_i &,if & Y_i\ge D\\
  D &,if & Y_i> D
\end{matrix}\right and 
$R_i$=\left\{\begin{matrix}
  1 &,if & Y_i\ge D\\
  0 &,if & Y_i< D
\end{matrix}\right, we have $X_i=Y_iR_i+(1-R_i)$, $X_i$ is left-truncated.

For non-truncated observations, when $x_i\ge D, x_i=y_i$, thus the contribution to the likelihood is $f_X(x_i)=f_Y(y_i)=\phi(y_i;\mu,\sigma^2)$.

For truncated observations $y_i<D$ then $x_i=D$, so the contribution to the likelihood is $Pr(x_i=D)=Pr(y_i<D)=\Phi(D;\mu,\sigma^2)=\phi(x_i;\mu,\sigma^2)$.

Assumed all observations are independent, we have the likelihood function for $\mu$ and $\sigma^2$ to be
\begin{align}
L(\mu,\sigma^2|\boldsymbol{x},\boldsymbol{r}) & = \prod_{i=1}^{n}[\phi(x_i;\mu,\sigma^2)]^{r_i}[\Phi(x_i;\mu,\sigma^2)]^{(1-r_i)}\\
&=\phi(x_i;\mu,\sigma^2)^{\sum_ir_i}\Phi(x_i;\mu,\sigma^2)^{\sum_i(1-r_i)}
\end{align}
Then we have the log-likelihood to be
$$
\log L(\mu,\sigma^2|\boldsymbol{x},\boldsymbol{r})=\sum_{i=1}^n\{r_i\log \phi(x_i;\mu,\sigma^2)+(1-r_i)\log\Phi(x_i;\mu,\sigma^2)\}
$$
## (b) Determine the maximum likelihood estimate of μ
Now we will build the log-likelihood function from (1) in `maxLik` to compute the maximum likelihood estimate of $\mu$ given $\sigma^2=1.5$.
```{r}
load("dataex3.Rdata")
require(maxLik)

# Build the likelihood function based on analysis in (a)
log_like <- function(param, data){
  x <- data[,1]
  r <- data[,2]
  mu <- param
  # normal distribution prob density function: dnorm(x,mu,sd)
  # normal distribution cumulative distribution function: pnorm()
  # Log-likelihood function, considering sigma=1.5
  sum(r*log(dnorm(x,mu,sd=1.5)) + (1-r)*log(pnorm(x,mu,sd=1.5)))
}

# Compute the maximum likelihood estimate
mle1 <- maxLik(logLik = log_like, data = dataex3, start = c(mu = 1))
summary(mle1)
# Change initial values but still got the same results
print("Change the initial values and cumputed again: ")
mle2 <- maxLik(logLik = log_like, data = dataex3, start = c(mu = -10))
mle2$estimate
mle3 <- maxLik(logLik = log_like, data = dataex3, start = c(mu = 10))
mle3$estimate
```
According to the outputs, the maximum likelihood estimate $\hat{\mu}$ is $5.532804$, with a significant p value. Changed the initial values for two more times, we still get the same estimate.

# Question 4
(a) Since logit$\{Pr(R=0|y_1,y_2,\boldsymbol{\theta},\boldsymbol{\psi})\}=\psi_0+\psi_1y_1$ indicating the missing value machenism only depends on fully observed $y_1$, the data are MAR.

Also, $\boldsymbol{\psi}=(\psi_0,\psi_1)$ distincts from $\boldsymbol{\theta}$, suggesting missing data mechanism doesn't contain information about parameters of the complete data model.

Combining these two conditions, this missing data mechanisms state is ignorable for likelihood-based estimation.


(b) Since logit$\{Pr(R=0|y_1,y_2,\boldsymbol{\theta},\boldsymbol{\psi})\}=\psi_0+\psi_1y_2$ depending on missing values $y_2$, the data are MNAR rather than MAR.

$\boldsymbol{\psi}=(\psi_0,\psi_1)$ distincts from $\boldsymbol{\theta}$, formally, implying the parameter space of ($\boldsymbol{\psi},\boldsymbol{\theta}$) is equal to the Cartesian product of their individual product space.

Thus, if we can impose $\psi_1=0$, this missing data mechanism will be ignorable for likelihood-based estimation, otherwise it won't.


(c) Considering logit$\{Pr(R=0|y_1,y_2,\boldsymbol{\theta},\boldsymbol{\psi})\}=0.5(\mu_1+\psi y_1)=0.5\mu_1+0.5\psi y_1$, the missing data only depend on fully observed $y_1$, so the data are MAR.

The missing data mechanism $\boldsymbol{\psi}=(\mu_1,\psi)$ is not disjoint with the data model $\boldsymbol{\theta}=(\mu_1,\mu_2,\sigma_{12},\sigma_1^2,\sigma_2^2)$ since $\mu_1$ is in both sets.

Thus, this missing data mechanism state is not ignorable for likelihood-based
estimation.

# Question 5
First we will have a look at the complete data likelihood.

Given $Y\stackrel{iid}{\sim}Bernoulli(p_i(\boldsymbol{\beta}))$, the likelihood function will be:
$$
L(\boldsymbol{\beta}|\boldsymbol{x},\boldsymbol{y})=\prod_{i=1}^{n}p_i(\boldsymbol{\beta})^{y_i}(1-p_i(\boldsymbol{\beta}))^{(1-y_i)} 
$$
Assuming the first $m$ data are observed and $m+1,...,n$ data are missing. Corresponding log-likehood function of complete data will be:
$$
\log L(\boldsymbol{\beta}|\boldsymbol{x},\boldsymbol{y})
=\sum_{i=1}^{m}y_i\log p_i(\boldsymbol{\beta})
+\sum_{i=1}^{m}(1-y_i)\log(1-p_i(\boldsymbol{\beta}))
+\sum_{i=m+1}^{n}y_i\log p_i(\boldsymbol{\beta})
+\sum_{i=m+1}^{n}(1-y_i)\log(1-p_i(\boldsymbol{\beta}))
$$
E-step:
\begin{align*}
Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}})
&=E_{Y_{mis}}[\log L(\boldsymbol{\beta})|\boldsymbol{y_{obs}},\boldsymbol{x},\boldsymbol{\beta}^{(t)}]\\
&=\sum_{i=1}^{m}y_i\log p_i(\boldsymbol{\beta})
+\sum_{i=1}^{m}(1-y_i)\log(1-p_i(\boldsymbol{\beta}))\\
&+E_{Y_{mis}}[\sum_{i=m+1}^{n}y_i\log p_i(\boldsymbol{\beta})
+\sum_{i=m+1}^{n}(1-y_i)\log(1-p_i(\boldsymbol{\beta}))|\boldsymbol{y_{obs}},\boldsymbol{x},\boldsymbol{\beta}^{(t)}]\\
\end{align*}
Further look at the expectation term, it's equal to
$$
\log p_i(\boldsymbol{\beta})\sum_{i=m+1}^{n}E_{Y_{mis}}[y_i|\boldsymbol{x},\boldsymbol{\beta}^{(t)}]
+\log(1-p_i(\boldsymbol{\beta}))\sum_{i=m+1}^{n}E_{Y_{mis}}[1-y_i|\boldsymbol{x},\boldsymbol{\beta}^{(t)}]
$$
Also, $E_{Y_{mis}}[y_i|\boldsymbol{x},\boldsymbol{\beta}^{(t)}]=p_i(\boldsymbol{\beta}^{(t)})$, $E_{Y_{mis}}[1-y_i|\boldsymbol{x},\boldsymbol{\beta}^{(t)}]=1-p_i(\boldsymbol{\beta}^{(t)})$ according to Bernoulli distribution properties.

Thus, we have the conditional expectation to be:
$$
Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}})=\sum_{i=1}^{m}y_i\log p_i(\boldsymbol{\beta})+\sum_{i=1}^{m}(1-y_i)\log (1-p_i(\boldsymbol{\beta}))\sum^{n}_{i=m+1}p_i(\boldsymbol{\beta}^{(t)})\log p_i(\boldsymbol{\beta})+\sum_{i=m+1}^{n}(1-p_i(\boldsymbol{\beta}^{(t)}))\log(1-p_i(\boldsymbol{\beta}))
$$
This equation doesn't have an explicit form of optimal solution, so we will use `optim` to solve it numerically. Later in the M-step, we will obtain $\boldsymbol{\beta^{(t+1)}}$ iteratively to maxmize $Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}})$.
```{r}
load("dataex5.Rdata")

# Define a function to calculate probability p(beta)
pbeta <-function(Beta,x){
  exp(Beta[1]+Beta[2]*x)/(1+exp(Beta[1]+Beta[2]*x))
}

### Preprocess the data set
# Data pairs including missing values
x_mis <- dataex5[which(is.na(dataex5[,2])),1]
# Data pairs without missing values
complete <- dataex5[-which(is.na(dataex5[,2])),]
x <- complete[,1]
y <- complete[,2]

# EM algorithm to estimate logit regression parameters
logit_EM <- function(initial_value, eps){

  # Set up initial value
  diff <- 1
  beta <- initial_value
  beta0t <- beta[1]
  beta1t <- beta[2]
  
  # E-M Algorithm
  while(diff > eps){
    beta.t <- beta
    
    # E-step: construct Q equation
    Q <- function(beta, betat){
      p_obs <- pbeta(beta, x)
      p_mis <- pbeta(beta, x_mis)
      p_mis.t <- pbeta(betat, x_mis)
      aim <- sum(y*log(p_obs) + (1-y)*log(1-p_obs))+
        sum(p_mis.t*log(p_mis) + (1-p_mis.t)*log(1-p_mis))
      return(-aim)
      }
    
    # M-step: maximum likelihood estimate of Q equation
    mle <- optim(par = beta, Q, betat = beta.t)
    beta <-  mle$par
    diff <- sum(abs(beta-beta.t))
  }
return(beta)
}

beta_EM <- logit_EM(initial_value = c(1, 1), eps = 0.000001)
print(paste("Estimates: beta0:", round(beta_EM[1],5), "beta1:", round(beta_EM[2],5)))
```
As seen in the output, $\boldsymbol{\hat{\beta}}=(\hat{\beta_0},\hat{\beta_1})=(0.97569,-2.48026)$

Github repo for the code file:\
<>



























